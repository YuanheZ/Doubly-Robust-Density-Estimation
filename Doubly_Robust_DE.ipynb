{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Environment"
      ],
      "metadata": {
        "id": "RzBdyoe2-Kwb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqnl6_PmDbSo"
      },
      "outputs": [],
      "source": [
        "# package for regular vine copula estimation\n",
        "!pip install pyvinecopulib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MS_vOcisDEvh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pyvinecopulib as pv\n",
        "from sklearn.model_selection import KFold\n",
        "import pickle\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import math\n",
        "from tqdm.notebook import tqdm\n",
        "import scipy.stats as stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inps_rPODWLI"
      },
      "outputs": [],
      "source": [
        "# package for linear interpolation based on pytorch\n",
        "!pip install xitorch\n",
        "import xitorch.interpolate as xi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNwpEwaDjpJ"
      },
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FTO2AlJZDm1I"
      },
      "outputs": [],
      "source": [
        "def cdf_lomax(x, a):\n",
        "  # cdf of lomax distribution with parameter a at x\n",
        "  return 1 - (1+x) ** (-a)\n",
        "\n",
        "def pdf_lomax(x, a):\n",
        "  # pdf of lomax distribution with parameter a at x\n",
        "  return a * (1 + x) **(-a-1)\n",
        "\n",
        "def alpha(step):\n",
        "  # alpha value derived by (Fong et al. 2021)\n",
        "  i = step\n",
        "  alpha = (2 - 1/i) * (1/(i+1))\n",
        "  return torch.tensor(alpha, dtype = torch.float32)\n",
        "\n",
        "def inverse_std_normal(cumulative_prob):\n",
        "\t'''\n",
        "\tInverse of the standard normal CDF.\n",
        "\t'''\n",
        "\tcumulative_prob_doube = cumulative_prob.double()\n",
        "\treturn torch.erfinv(2 * cumulative_prob_doube - 1) * torch.sqrt(torch.tensor(2.0))\n",
        "\n",
        "def cdf_std_normal(input):\n",
        "  '''\n",
        "\tcdf of the standard normal CDF.\n",
        "\t'''\n",
        "  return torch.distributions.normal.Normal(loc = 0, scale = 1).cdf(input)\n",
        "\n",
        "def pdf_std_normal(input):\n",
        "  '''\n",
        "\tpdf of the standard normal CDF.\n",
        "\t'''\n",
        "  return torch.distributions.normal.Normal(loc = 0, scale = 1).log_prob(input).exp()\n",
        "\n",
        "def bvn_density(rho, u, v, shift = 0.0, scale = 1.0):\n",
        "\n",
        "  if len(u) != len(v):\n",
        "    print('Error: length of u and v should be equal')\n",
        "  else:\n",
        "   mean = torch.tensor([shift, shift])\n",
        "   covariance_matrix = torch.tensor([[scale, rho], [rho, scale]])\n",
        "   multivariate_normal = torch.distributions.MultivariateNormal(mean, covariance_matrix)\n",
        "\n",
        "   l = len(u)\n",
        "   input = torch.cat([u.reshape(l, 1), v.reshape(l, 1)], dim=1)\n",
        "\n",
        "   return multivariate_normal.log_prob(inverse_std_normal(input)).exp()\n",
        "\n",
        "def GC_density(rho, u, v, shift = 0.0, scale = 1.0):\n",
        "\n",
        "  v_d = pdf_std_normal(inverse_std_normal(v)).reshape(len(v), 1)\n",
        "  u_d = pdf_std_normal(inverse_std_normal(u)).reshape(len(u), 1)\n",
        "  low = u_d * v_d\n",
        "\n",
        "  up = bvn_density(rho = rho, u = u, v = v).reshape(len(u), 1)\n",
        "\n",
        "  return up / low\n",
        "\n",
        "def cbvn_density(rho, u, v, shift = 0.0, scale = 1.0):\n",
        "\n",
        "   mean = torch.tensor([shift, shift])\n",
        "   covariance_matrix = torch.tensor([[scale, rho], [rho, scale]])\n",
        "   multivariate_normal = torch.distributions.MultivariateNormal(mean, covariance_matrix)\n",
        "\n",
        "   l = len(u)\n",
        "   input = torch.cat([u.reshape(l, 1), v * torch.ones(l, 1)], dim=1)\n",
        "\n",
        "   return multivariate_normal.log_prob(inverse_std_normal(input)).exp()\n",
        "\n",
        "def cGC_density(rho, u, v, shift = 0.0, scale = 1.0):\n",
        "  '''\n",
        "\tpdf of the Gaussian copula at u conditional on v\n",
        "\t'''\n",
        "\n",
        "  l = len(u)\n",
        "\n",
        "  v_d = pdf_std_normal(inverse_std_normal(v))\n",
        "  u_d = pdf_std_normal(inverse_std_normal(u)).reshape(l, 1)\n",
        "  low = u_d * v_d\n",
        "\n",
        "  up = cbvn_density(rho = rho, u = u, v = v).reshape(l, 1)\n",
        "\n",
        "  return up / low\n",
        "\n",
        "def cGC_distribution(rho, u, v, shift = 0.0, scale = 1.0):\n",
        "  '''\n",
        "\tcdf of the Gaussian copula at u conditional on v\n",
        "\t'''\n",
        "  upper = inverse_std_normal(u).reshape(len(u), 1) - rho * inverse_std_normal(v)\n",
        "  lower = torch.sqrt(torch.tensor(1 - rho ** 2))\n",
        "  input = upper / lower\n",
        "\n",
        "  return cdf_std_normal(input)\n",
        "\n",
        "def drop_corr(y,threshold= 0.98):\n",
        "    # Drop highly correlated variables\n",
        "    data = pd.DataFrame(y)\n",
        "    # Create correlation matrix\n",
        "    corr_matrix = data.corr().abs()\n",
        "\n",
        "    # Select upper triangle of correlation matrix\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "    # Find index of feature columns with correlation greater than 0.95\n",
        "    to_drop = [col for col in upper.columns if any(upper[col] > threshold)]\n",
        "    y = data.drop(columns = to_drop).values\n",
        "\n",
        "    return(y)\n",
        "\n",
        "def create_permutatons(obs, perms):\n",
        "  # create random permutations for the training set\n",
        "  permutations = []\n",
        "  L = obs.shape[0]\n",
        "\n",
        "  for _ in range(perms):\n",
        "\n",
        "    permutation = torch.randperm(L)\n",
        "    sequence = obs[permutation, :]\n",
        "    permutations.append(sequence)\n",
        "\n",
        "  return torch.stack(permutations)\n",
        "\n",
        "def Energy_Score_pytorch(beta, observations_y, simulations_Y):\n",
        "        # compute the energy score given observations and simulations\n",
        "\n",
        "        n = len(observations_y)\n",
        "        m = len(simulations_Y)\n",
        "\n",
        "        # First part |Y-y|. Gives the L2 dist scaled by power beta. Is a vector of length n/one value per location.\n",
        "        diff_Y_y = torch.pow(\n",
        "            torch.norm(\n",
        "                (observations_y.unsqueeze(1) -\n",
        "                simulations_Y.unsqueeze(0)).float(),\n",
        "                dim=2,keepdim=True).reshape(-1,1),\n",
        "            beta)\n",
        "\n",
        "        # Second part |Y-Y'|. 2* because pdist counts only once.\n",
        "        diff_Y_Y = 2 * torch.pow(\n",
        "            nn.functional.pdist(simulations_Y),\n",
        "            beta)\n",
        "        Energy = 2 * torch.mean(diff_Y_y) - torch.sum(diff_Y_Y) / (m * (m - 1))\n",
        "        return Energy\n",
        "\n",
        "def minmax_unif(obs):\n",
        "  '''\n",
        "  An informative uniform prior whose support is same as data's\n",
        "  '''\n",
        "  min = torch.min(obs) - 0.001\n",
        "  max = torch.max(obs) + 0.001\n",
        "  log_pdfs = torch.distributions.uniform.Uniform(min, max).log_prob(obs)\n",
        "  cdfs = torch.distributions.uniform.Uniform(min, max).cdf(obs)\n",
        "  return cdfs, log_pdfs.exp()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "k_if_9X6Ax7m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "83h58DhbDu5C"
      },
      "outputs": [],
      "source": [
        "def preq_train_rho(observations, size, init_dist = 'Normal', a = 1., up = 0.99, low = 0.1):\n",
        "  '''\n",
        "  prequential training approach via grid search in (Fong et al. 2023 JRSSB)\n",
        "  ---\n",
        "  Input:\n",
        "  observations: permuted training data\n",
        "  size: the number of grids we want to search\n",
        "  init_dist: initial distribution\n",
        "  a: parameter of lomax distribution\n",
        "  up: the upper bound of grid seach\n",
        "  low: the lower bound of grid search\n",
        "  ---\n",
        "  Output:\n",
        "  optimal rho values for each dimension\n",
        "  '''\n",
        "\n",
        "  num_perm = observations.shape[0]\n",
        "  num_data = observations.shape[1]\n",
        "  num_dim = observations.shape[2]\n",
        "\n",
        "  opt_rhovec = torch.zeros([num_dim])\n",
        "  theta_grids = torch.linspace(low, up, size)\n",
        "\n",
        "  for j in tqdm(range(num_dim)):\n",
        "\n",
        "    losses = torch.zeros([size])\n",
        "\n",
        "    for step in range(size):\n",
        "\n",
        "      pl = torch.zeros([num_perm])\n",
        "\n",
        "      for perm in range(num_perm):\n",
        "\n",
        "        if init_dist == 'Normal':\n",
        "\n",
        "          cdf = torch.distributions.normal.Normal(loc=0, scale=1).cdf(observations[perm,:,j]).reshape(num_data)\n",
        "          pdf = torch.distributions.normal.Normal(loc=0, scale=1).log_prob(observations[perm,:,j]).exp().reshape(num_data)\n",
        "\n",
        "        if init_dist == 'Cauchy':\n",
        "\n",
        "          cdf = torch.distributions.cauchy.Cauchy(loc=0.0, scale=1.0).cdf(observations[perm,:,j]).reshape(num_data)\n",
        "          pdf = torch.distributions.cauchy.Cauchy(loc=0.0, scale=1.0).log_prob(observations[perm,:,j]).exp().reshape(num_data)\n",
        "\n",
        "        if init_dist == 'Lomax':\n",
        "\n",
        "          cdf = cdf_lomax(observations[perm,:,j], a)\n",
        "          pdf = pdf_lomax(observations[perm,:,j], a)\n",
        "\n",
        "        for k in range(1, num_data):\n",
        "          Cop = cGC_distribution(rho = theta_grids[step], u = cdf[1:], v = cdf[0]).reshape(num_data-k)\n",
        "          cop = cGC_density(rho = theta_grids[step], u = cdf[1:], v = cdf[0]).reshape(num_data-k)\n",
        "          cdf = (1 - alpha(k)) * cdf[1:] + alpha(k) * Cop\n",
        "          pdf = (1 - alpha(k)) * pdf[1:] + alpha(k) * cop * pdf[1:]\n",
        "\n",
        "          pl[perm] = pl[perm] + torch.log(pdf[0])\n",
        "\n",
        "        losses[step] = torch.mean(pl)\n",
        "\n",
        "    opt_rhovec[j] = theta_grids[torch.argmax(losses)]\n",
        "\n",
        "  return opt_rhovec\n",
        "\n",
        "def get_context(observations, rhovec, init_dist = 'Cauchy', a = 1.):\n",
        "    '''\n",
        "    Compute predictive cdf values of training data\n",
        "    ---\n",
        "    Input:\n",
        "    observations: permuted training data\n",
        "    rhovec: the rho values for each dimension\n",
        "    init_dist: initial distribution\n",
        "    a: parameter of lomax distribution\n",
        "    ---\n",
        "    Output:\n",
        "    predictive cdf values of training data\n",
        "    '''\n",
        "    flt = 1e-6\n",
        "\n",
        "    num_perm = observations.shape[0]\n",
        "    num_data = observations.shape[1]\n",
        "    num_dim = observations.shape[2]\n",
        "\n",
        "    context = torch.zeros([num_perm, num_data, num_dim])\n",
        "\n",
        "    for j in range(num_dim):\n",
        "\n",
        "      for perm in range(num_perm):\n",
        "\n",
        "        if init_dist == 'Normal':\n",
        "\n",
        "          cdf = torch.distributions.normal.Normal(loc=0, scale=1).cdf(observations[perm,:,j]).reshape(num_data)\n",
        "\n",
        "        if init_dist == 'Cauchy':\n",
        "\n",
        "          cdf = torch.distributions.cauchy.Cauchy(loc=0.0, scale=1.0).cdf(observations[perm,:,j]).reshape(num_data)\n",
        "\n",
        "        if init_dist == 'Lomax':\n",
        "\n",
        "          cdf = cdf_lomax(observations[perm,:,j], a)\n",
        "\n",
        "        if init_dist == 'Unif':\n",
        "\n",
        "          cdf, _ = minmax_unif(observations[perm,:,j].reshape(num_data))\n",
        "\n",
        "        cdf = torch.clip(cdf, min=flt, max=1.+flt)\n",
        "\n",
        "        context[perm, 0, j] = cdf[0]\n",
        "\n",
        "        for k in range(1, num_data):\n",
        "\n",
        "          Cop = cGC_distribution(rho = rhovec[j], u = cdf[1:], v = cdf[0]).reshape(num_data-k)\n",
        "          cdf = (1 - alpha(k)) * cdf[1:] + alpha(k) * Cop\n",
        "          cdf = torch.clip(cdf, min=flt, max=1.+flt)\n",
        "          context[perm, k, j] = cdf[0]\n",
        "\n",
        "    return context\n",
        "\n",
        "def evaluate_prcopula(test_points, context, rhovec, init_dist = 'Cauchy', a = 1.):\n",
        "      '''\n",
        "      Evaluate cdf and pdf values at test points\n",
        "      ---\n",
        "      Input:\n",
        "      test_points: test data\n",
        "      context: cdf values for training data\n",
        "      rhovec: the rho values for each dimension\n",
        "      init_dist: initial distribution\n",
        "      a: parameter of lomax distribution\n",
        "      ---\n",
        "      Output:\n",
        "      predictive pdf and cdf values at test points\n",
        "      '''\n",
        "\n",
        "      flt = 1e-6\n",
        "\n",
        "      num_evals = test_points.shape[0]\n",
        "      num_perm = context.shape[0]\n",
        "      num_data = context.shape[1]\n",
        "      num_dim = test_points.shape[1]\n",
        "\n",
        "      dens = torch.zeros([num_perm, num_evals, num_dim])\n",
        "      cdfs = torch.zeros([num_perm, num_evals, num_dim])\n",
        "\n",
        "      for j in tqdm(range(num_dim)):\n",
        "\n",
        "        for perm in range(num_perm):\n",
        "\n",
        "            if init_dist == 'Normal':\n",
        "\n",
        "                cdf = torch.distributions.normal.Normal(loc=0, scale=1).cdf(test_points[:,j]).reshape(num_evals)\n",
        "                pdf = torch.distributions.normal.Normal(loc=0, scale=1).log_prob(test_points[:,j]).exp().reshape(num_evals)\n",
        "\n",
        "            if init_dist == 'Cauchy':\n",
        "\n",
        "                cdf = torch.distributions.cauchy.Cauchy(loc=0.0, scale=1.0).cdf(test_points[:,j]).reshape(num_evals)\n",
        "                pdf = torch.distributions.cauchy.Cauchy(loc=0.0, scale=1.0).log_prob(test_points[:,j]).exp().reshape(num_evals)\n",
        "\n",
        "            if init_dist == 'Lomax':\n",
        "\n",
        "                cdf = cdf_lomax(test_points[:,j], a)\n",
        "                pdf = pdf_lomax(test_points[:,j], a)\n",
        "\n",
        "            if init_dist == 'Unif':\n",
        "\n",
        "                cdf, pdf = minmax_unif(test_points[:,j].reshape(num_evals))\n",
        "\n",
        "            cdf = torch.clip(cdf, min=flt, max=1.+flt)\n",
        "\n",
        "            for k in range(0, num_data):\n",
        "\n",
        "                cop = cGC_density(rho = rhovec[j], u = cdf, v = context[perm, k, j]).reshape(num_evals)\n",
        "                Cop = cGC_distribution(rho = rhovec[j], u = cdf, v = context[perm, k, j]).reshape(num_evals)\n",
        "                cdf = (1 - alpha(k+1)) * cdf + alpha(k+1) * Cop\n",
        "                cdf = torch.clip(cdf, min=flt, max=1.+flt)\n",
        "                pdf = (1 - alpha(k+1)) * pdf + alpha(k+1) * cop * pdf\n",
        "\n",
        "            dens[perm, :, j] = pdf\n",
        "            cdfs[perm, :, j] = cdf\n",
        "\n",
        "      return torch.mean(dens, dim=0), torch.mean(cdfs, dim=0)\n",
        "\n",
        "def grids_cdfs(size, context, rhovec, data, extrap_tail = .1, init_dist = 'Cauchy', a = 1.):\n",
        "      '''\n",
        "      Evaluate cdf values on a grid of points\n",
        "      ---\n",
        "      Input:\n",
        "      size: the number of grid points\n",
        "      context: cdf values for training data\n",
        "      rhovec: the rho values for each dimension\n",
        "      data: normalized original training set (not permuted)\n",
        "      extrap_tail: the range of extrapolation outside the support of data\n",
        "      init_dist: initial distribution\n",
        "      a: parameter of lomax distribution\n",
        "      ---\n",
        "      Output:\n",
        "      grid of points matrix, cdf values on a grid of points\n",
        "      '''\n",
        "\n",
        "      flt = 1e-6\n",
        "\n",
        "      num_perm = context.shape[0]\n",
        "      num_data = context.shape[1]\n",
        "      num_dim = context.shape[2]\n",
        "\n",
        "      gridmat = torch.zeros([size, num_dim])\n",
        "\n",
        "      cdfs = torch.zeros([num_perm, size, num_dim])\n",
        "\n",
        "      for j in range(num_dim):\n",
        "\n",
        "        min = torch.min(data[:,j]) - extrap_tail\n",
        "        max = torch.max(data[:,j]) + extrap_tail\n",
        "        xgrids = torch.linspace(min, max, size)\n",
        "        gridmat[:,j] = xgrids\n",
        "\n",
        "        for perm in range(num_perm):\n",
        "\n",
        "            if init_dist == 'Normal':\n",
        "\n",
        "                cdf = torch.distributions.normal.Normal(loc=0, scale=1).cdf(xgrids).reshape(size)\n",
        "\n",
        "            if init_dist == 'Cauchy':\n",
        "\n",
        "                cdf = torch.distributions.cauchy.Cauchy(loc=0.0, scale=1.0).cdf(xgrids).reshape(size)\n",
        "\n",
        "            if init_dist == 'Lomax':\n",
        "\n",
        "                cdf = cdf_lomax(xgrids, a)\n",
        "\n",
        "            if init_dist == 'Unif':\n",
        "\n",
        "                cdf, _ = minmax_unif(xgrids.reshape(size))\n",
        "\n",
        "            cdf = torch.clip(cdf, min=flt, max=1.+flt)\n",
        "\n",
        "            for k in range(0, num_data):\n",
        "\n",
        "                Cop = cGC_distribution(rho = rhovec[j], u = cdf, v = context[perm, k, j]).reshape(size)\n",
        "                cdf = (1 - alpha(k+1)) * cdf + alpha(k+1) * Cop\n",
        "                cdf = torch.clip(cdf, min=flt, max=1.+flt)\n",
        "\n",
        "            cdfs[perm, :, j] = cdf\n",
        "\n",
        "      return gridmat, torch.mean(cdfs, dim=0)\n",
        "\n",
        "def linear_energy_grid_search(observations, rhovec, beta = 0.5, size = 1000, evalsz = 100, extrap_tail = .1, extrap_bound = .5, init_dist = 'Normal', a = 1.):\n",
        "  '''\n",
        "  Grid search optimization by Energy Score\n",
        "  ---\n",
        "  Input:\n",
        "  observations: permuted training data\n",
        "  rhovec: the rho values we want to search\n",
        "  beta: scale parameter of Energy Score\n",
        "  size: the size of context set to approximate the inverse cdf by linear interpolation\n",
        "  evalsz: size of resampling from predictive\n",
        "  extrap_tail: the range of extrapolation outside the support of data\n",
        "  extrap_bound: pseudo extra bounds for linear interpolation (manually set the upper/lower bound to be 0/1)\n",
        "  init_dist: initial distribution\n",
        "  a: parameter of lomax distribution\n",
        "  ---\n",
        "  Output:\n",
        "  records of energy score at each rho value for each dimension\n",
        "  '''\n",
        "  ctxtmat = get_context(observations, rhovec, init_dist, a)\n",
        "  gridmatrix, gridcdf = grids_cdfs(size, ctxtmat, rhovec, observations, extrap_tail, init_dist, a)\n",
        "  sams = torch.rand([evalsz, observations.shape[2]])\n",
        "  scores = torch.zeros([observations.shape[2]])\n",
        "  for dim in range(observations.shape[2]):\n",
        "    lcb = torch.min(gridmatrix[:,dim].reshape([gridmatrix.shape[0]])) - extrap_bound\n",
        "    ucb = torch.max(gridmatrix[:,dim].reshape([gridmatrix.shape[0]])) + extrap_bound\n",
        "    sorted_grids = torch.cat([lcb.unsqueeze(0), gridmatrix[:,dim].reshape([gridmatrix.shape[0]]), ucb.unsqueeze(0)])\n",
        "    cdf_values = torch.cat([torch.tensor(0.0).unsqueeze(0), gridcdf[:,dim].reshape([gridcdf.shape[0]]), torch.tensor(1.0).unsqueeze(0)])\n",
        "    inv = xi.Interp1D(cdf_values, sorted_grids, method=\"linear\")\n",
        "    scores[dim] = Energy_Score_pytorch(beta, observations[0, :, dim].reshape([observations.shape[1], 1]), inv(sams[:,dim]).reshape([evalsz, 1]))\n",
        "\n",
        "  return scores\n",
        "\n",
        "def extract_grids_search(scores, lower = 0.1, upper = 0.99):\n",
        "  '''\n",
        "  Get the optimal theta for each marginal given records of energy score for each grid rho value from linear_energy_grid_search\n",
        "  '''\n",
        "  size = scores.shape[0]\n",
        "  num_dim = scores.shape[1]\n",
        "  theta_dic = torch.linspace(lower, upper, size)\n",
        "  optimums = torch.zeros([num_dim])\n",
        "  for dim in range(num_dim):\n",
        "    interim = scores[:,dim].reshape([size])\n",
        "    optimums[dim] = theta_dic[torch.argmin(interim)]\n",
        "\n",
        "  return optimums\n",
        "\n",
        "def linvsampling(observations, context, sams, rhovec, beta = 0.5, approx = 1000, extrap_tail = .1, extrap_bound = .5, init_dist = 'Normal', a = 1.):\n",
        "  '''\n",
        "  Global sampling for estimated density given fitted regular vine copula samples via linear interpolation\n",
        "  ---\n",
        "  Input:\n",
        "  observations: permuted training data\n",
        "  context: cdf values for training data\n",
        "  sams: copula samples from the regular vine copula\n",
        "  rhovec: the optimal rho values for each dimension\n",
        "  beta: scale parameter of Energy Score\n",
        "  approx: the size of context set to approximate the inverse cdf by linear interpolation\n",
        "  extrap_tail: the range of extrapolation outside the support of data\n",
        "  extrap_bound: pseudo extra bounds for linear interpolation (manually set the upper/lower bound to be 0/1)\n",
        "  init_dist: initial distribution\n",
        "  a: parameter of lomax distribution\n",
        "  ---\n",
        "  Output:\n",
        "  samples from the joint estimated density\n",
        "  '''\n",
        "  gridmatrix, gridcdf = grids_cdfs(approx, context, rhovec, observations, extrap_tail, init_dist, a)\n",
        "  for dim in range(observations.shape[2]):\n",
        "    lcb = torch.min(gridmatrix[:,dim].reshape([gridmatrix.shape[0]])) - extrap_bound\n",
        "    ucb = torch.max(gridmatrix[:,dim].reshape([gridmatrix.shape[0]])) + extrap_bound\n",
        "    sorted_grids = torch.cat([lcb.unsqueeze(0), gridmatrix[:,dim].reshape([gridmatrix.shape[0]]), ucb.unsqueeze(0)])\n",
        "    cdf_values = torch.cat([torch.tensor(0.0).unsqueeze(0), gridcdf[:,dim].reshape([gridcdf.shape[0]]), torch.tensor(1.0).unsqueeze(0)])\n",
        "    inv = xi.Interp1D(cdf_values, sorted_grids, method=\"linear\")\n",
        "    sams[:,dim] = inv(sams[:,dim])\n",
        "\n",
        "  return sams\n",
        "\n",
        "def energy_cv(data, K, up = 4., low = 2., size = 10, beta = .5):\n",
        "  '''\n",
        "  Bandwidth selection for kde regular vine copula via K-Fold Energy Score Cross Validation\n",
        "  ---\n",
        "  Input:\n",
        "  data: fitted cdf values of training data (pesudo training copula samples)\n",
        "  K: the number of folds\n",
        "  up: the upper bound of bandwidth value to search\n",
        "  low: the lower bound of bandwidth value to search\n",
        "  size: the number of bandwidth values to search\n",
        "  beta: scale parameter of Energy Score\n",
        "  ---\n",
        "  Output:\n",
        "  optimal bandwidth\n",
        "  '''\n",
        "  kfold = KFold(n_splits=K, random_state=100, shuffle=True)\n",
        "  bgrids = np.linspace(low, up, size)\n",
        "  in_sample = torch.zeros([size, K])\n",
        "  for train, test in kfold.split(data):\n",
        "    i = 0\n",
        "    for epoch in tqdm(range(size)):\n",
        "      controls = pv.FitControlsVinecop(family_set=[pv.BicopFamily.tll], selection_criterion='mbic', nonparametric_method='constant', nonparametric_mult=bgrids[epoch], num_threads = 2048)\n",
        "      cop = pv.Vinecop(data[train], controls=controls)\n",
        "      news = cop.simulate(100)\n",
        "      in_sample[epoch, i] = Energy_Score_pytorch(beta, data[test], torch.tensor(news, dtype=torch.float32))\n",
        "    i = i + 1\n",
        "  in_sample_err = torch.mean(in_sample, dim=1)\n",
        "  return bgrids[torch.argmin(in_sample_err)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ap7Eu-wQD_7r"
      },
      "outputs": [],
      "source": [
        "# main model\n",
        "def fit_eval_sampl_DBDE(data,p0_class,n_new):\n",
        "    '''\n",
        "    Fitting, evaluation, and sampling for doubly robust density estimator\n",
        "    ---\n",
        "    Input:\n",
        "    data = pd.read_csv(...)\n",
        "    p0_class = 'Normal' or 'Cauchy' or 'Lomax' or 'Unif'\n",
        "    n_new: the number of the new generations from estimator\n",
        "    ---\n",
        "    Output:\n",
        "    nll: negative log-likelihood on test set\n",
        "    opt: optimal rho values for each dimension\n",
        "    optband: optimal bandwidth for kde rvine copula\n",
        "    new_data: new generations from estimator\n",
        "    '''\n",
        "\n",
        "    ### Data processing\n",
        "    print('Data processing....................................................')\n",
        "    data = drop_corr(data)\n",
        "    frac_train = 0.5 # train-test splitting ratio\n",
        "    perms = 10 # number of permutation\n",
        "\n",
        "    n_tot = np.shape(data)[0]\n",
        "    n = int(frac_train*n_tot)\n",
        "\n",
        "    train_ind, test_ind = train_test_split(np.arange(n_tot),test_size = n_tot - n,train_size = n,random_state = 0)\n",
        "\n",
        "    y = torch.tensor(data[train_ind], dtype=torch.float32)\n",
        "    mean_y = torch.mean(y,axis = 0)\n",
        "    std_y = torch.std(y,axis = 0)\n",
        "    y = (y-mean_y)/std_y\n",
        "\n",
        "    y_permutations = create_permutatons(y, perms)\n",
        "\n",
        "    y_test = torch.tensor(data[test_ind], dtype=torch.float32)\n",
        "    y_test = (y_test-mean_y)/std_y\n",
        "\n",
        "    ### Training for M=marginal\n",
        "    print('Training for M=marginal....................................................')\n",
        "    size = 50\n",
        "    theta_grids = torch.linspace(0.1, 0.99, size) # define the range and size of thetas you want to search\n",
        "    scores_dic = torch.zeros([size, y.shape[1]]) # record energy scores for each theta\n",
        "\n",
        "    for grids in tqdm(range(size)):\n",
        "        scores_dic[grids, :] = linear_energy_grid_search(y_permutations,\n",
        "                                                        torch.linspace(theta_grids[grids], theta_grids[grids], y.shape[1]),\n",
        "                                                        beta = .5,\n",
        "                                                        init_dist = p0_class)\n",
        "\n",
        "    opt = extract_grids_search(scores_dic, lower = 0.1, upper = 0.99)\n",
        "    print('optimal rho values for each dimension: ',opt)\n",
        "\n",
        "    ### Training for copula\n",
        "    print('Training for copula....................................................')\n",
        "    optimum = opt\n",
        "    ctxt = get_context(y_permutations, optimum, init_dist=p0_class)\n",
        "    _, pseudos = evaluate_prcopula(y, ctxt, optimum, init_dist=p0_class) # construct copula values\n",
        "    print('K-Fold Cross Validation....................................................')\n",
        "    optband = energy_cv(pseudos, K = 10, up = 4., low = 2., size = 50) # select the bandwidth by energy score-based k-fold cross-validation\n",
        "    print('optimal bandwidth for kde regular vine copula: ',optband)\n",
        "    #fit the rvine copula\n",
        "    controls = pv.FitControlsVinecop(family_set=[pv.BicopFamily.tll],\n",
        "                                    selection_criterion='mbic',\n",
        "                                    nonparametric_method='constant', #KDE-copula\n",
        "                                    nonparametric_mult=optband, # bandwidth\n",
        "                                    num_threads = 2048)\n",
        "    cop = pv.Vinecop(pseudos, controls=controls)\n",
        "\n",
        "    ### Evaluation\n",
        "    print('Evaluation....................................................')\n",
        "    test_dens, test_cdfs = evaluate_prcopula(y_test, ctxt, optimum, init_dist=p0_class)\n",
        "    cop_dens = torch.tensor(cop.loglik(test_cdfs), dtype=torch.float32)\n",
        "    nll = -torch.mean(torch.sum(torch.log(test_dens), dim=1)) - (cop_dens / (n_tot - n))\n",
        "    print('the nll on test data: ',nll)\n",
        "\n",
        "    ### Sampling\n",
        "    print('Sampling....................................................')\n",
        "    sams = cop.simulate(n_new)\n",
        "    sams = torch.tensor(sams, dtype=torch.float32)\n",
        "    new_data = linvsampling(y_permutations, ctxt, sams, optimum, init_dist=p0_class)\n",
        "    new_data = new_data * std_y + mean_y\n",
        "\n",
        "    return nll, opt, optband, new_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaP84_D_ETnr"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXN88cLtEWV8"
      },
      "outputs": [],
      "source": [
        "# 5 runs\n",
        "for _ in range(5):\n",
        "   nll, opt, optband, new_data = fit_eval_sampl_DBDE(pd.read_csv('/content/breast.data'), 'Cauchy', 100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RzBdyoe2-Kwb",
        "HMNwpEwaDjpJ",
        "k_if_9X6Ax7m",
        "TaP84_D_ETnr"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}